## 모각코 시간
2022-02-07 16:40 ~ 2022-02-07 

## 공부한 내용
### Stochastic Gradient Descent(확률적 경사 하강법, SGD)
경사 하강법이 매 스텝마다 전체 훈련 세트를 사용해서 Gradient를 계산하는 반면에 확률적 경사 하강법은 매 스텝마다 한 개의 샘플을 무작위로 선택하여 그 샘플에 대한 Gradient를 계산한다.
1. 매 반복에서 적은 양의 데이터로 Gradient를 계산하고 업데이트 하기 때문에 최적화가 더 빠르다.
2. 매우 큰 train set도 처리할 수 있다.
3. 무작위 추출이기 때문에 전체 데이터를 사용하는 것보다 안정적이지 못하다.
    - 무작위 추출이기 때문에 random_state 매개변수를 설정해주어야 한다.
4. 비용 함수의 global minimum에 도달하기 까지 요동치며 평균적으로 감소한다.
    - 최솟값에 도달하지 않을 수도 있다.
5. 비용함수가 MSE처럼 볼록하지 않고 불균형하다면 배치 경사 하강법보다 global minimum에 도달할 가능성이 높다.
6. 무작위성으로 인한 global minimum에 도달하지 않을 수 있다는 단점을 극복하기 위해서 학습률을 점진적으로 감소시키는 해결책이 있다.
    - 시작: 학습률 크게, 진행단계: 학습률 작게

### StratifiedKFold 사용할 때 KeyError: "None of [Int64Index([0, 1, 2,...])] are in the [columns]"
인덱싱에 문제가 있는 것 같아서 인덱스 기호 대신에 .iloc를 통해서 접근하려고 했다. 기본적으로 SKFold객체로 split 했을 때 받는 값이 NumPy array라서 index열에 해당하는 위치에 넣어주면 될 것 같았다.
근데 X_train에는 .iloc로 접근이 가능했는데, y_train_5에는 접근이 안되고 "too many indexer"라는 에러가 떴다.
그래서 이 경우에는 .loc로 접근하고 열에 해당하는 부분은 싹 지웠더니 해결되었다.

### 오차행렬(Confusion matrix)
cross_val_predict()로 교차 검증을 통한 예측값을 반환하여 그 값을 confusion_matirx함수로 오차행렬을 만들었다.
오차행렬의 행은 실제 클래스를 나타내고 열은 예측한 클래스를 나타낸다고 한다.
첫 번째 행은 음성 클래스(negative class), 두 번째 행은 양성 클래스(positive class)이다.
첫 번째 열은 true이며 두 번째 열은 false이다.
즉, 제 1종오류, 2종오류와 비슷한 맥락인 듯하다.

|예측|음성|양성|
|--|--|--|
|음성|TN|FP|
|양성|FN|TP|

