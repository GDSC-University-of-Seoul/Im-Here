## 모각코 시간
2020-01-27 15:45 ~ 2020-01-27 17:30

## 공부한 내용
### 무작위 샘플링과 계층 샘플링 비교
오늘 공부한 housing 데이터 셋에서의 샘플링 오류율을 나타낸 것이다. 확실히 계층별로 차이가 있을 때 계층을 잘 나누어 샘플링을 하는 것이 오류율을 더 줄일 수 있는 방법이었다.

||전체|계층 샘플링|무작위 샘플링|무작위 샘플링 오류율|계층 샘플링 오류율|
|-|-|-|-|-|-|
|1|0.039826|0.039729|0.040213|0.973236|-0.243309|
|2|0.318847|0.318798|0.324370|1.732260|-0.015195|
|3|0.350581|0.350533|0.358527|2.266446|-0.013820|
|4|0.176308|0.176357|0.167393|-5.056334|0.027480|
|5|0.114438|0.114583|0.109496|-4.318374|0.127011|

### SimpleImputer
결측치를 대체하는 방법으로 사이킷런의 SimpleImputer를 공부했다. 이게 너무 좋은게 이때까지는 각 feature 하나하나당 NaN값 찾고 그걸 뭐로 대체할 지 정해서 feature 하나하나 다 적용해줬어야 했는데, SimpleImputer를 사용하면 한 방에 해결되는거였다.
```
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy = "median")
# median은 수치형 데이터에만 해당되기 때문에 object 데이터인 ocean_proximity는 제거하는 모습이다.
housing_num = housing.drop("ocean_proximity", axis = 1)

# 데이터 셋에 적용하는 모습
imputer.fit(housing_num)
```

### KNN Imputer
SimpleImputer를 공부하던 도중 책 맨 밑에 각주로 사이킷런 0.22 버전 이상에서는 KNN 알고리즘 방식으로 누락된 값을 대체하는 KNN Imputer가 있다고 해서 공부해봤다.

n_neighbors parameter에 값을 설정한 뒤 각 feature의 가장 가까운 이웃들을 모아 평균값을 낸다.
기본적으로 Euclidean distance를 사용하여 이웃간 거리를 구한다.

